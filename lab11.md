人工智能时代加速到来，算法决策兴起

![在这里插入图片描述](https://img-blog.csdnimg.cn/20181230225037773.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzI2ODM5Mw==,size_16,color_FFFFFF,t_70)


第三次AI（人工智能，以下简称AI）浪潮已经开启。在技术层面，有算法的进步。
当1956年人工智能开始起步的时候，人们更多是在说人工智能；在第二次浪潮期
间，机器学习成为主流；这一次则是深度学习，是能够自我学习、自我编程的学习
算法，可以用来解决更复杂的任务。此外，计算能力的提升，包括现在的量子计算
机，以及越来越普遍的大数据，对人工智能的作用和价值也非常大，使得更复杂的
算法成为可能。在应用层面，从语音识别、机器翻译到医疗诊断、自动驾驶，AI应
用在不断加深、不断成熟，甚至已经开始超越人类，引发人们关于失业的担忧。同
时也让人们开始期待具有通用智能的终极算法。

人工智能伦理问题也日益凸显
 以算法歧视为例：
     算法是一种数学表达，是很客观的，不像人类那样有各种偏见、情绪，容易受
     外部因素影响，怎么会产生歧视呢？之前的一些研究表明，法官在饿着肚子的
     时候，倾向于对犯罪人比较严厉，判刑也比较重，所以人们常说，正义取决于
     法官有没有吃早餐。算法也正在带来类似的歧视问题。比如，一些图像识别软
     件之前还将黑人错误地标记为“黑猩猩”或者“猿猴”。
   
   此外，2016年3月，微软公司在美国的Twitter上上线的聊天机器人Tay在与网
   民互动过程中，成为了一个集性别歧视、种族歧视等于一身的“不良少女”。随着
   算法决策越来越多，类似的歧视也会越来越多。而且，算法歧视会带来危害。一
   方面，如果将算法应用在犯罪评估、信用贷款、雇佣评估等关切人身利益的场
   合，一旦产生歧视，必然危害个人权益。另一方面，深度学习是一个典型的“黑
   箱”算法，连设计者可能都不知道算法如何决策，要在系统中发现有没有存在歧
   视和歧视根源，在技术上是比较困难的。
   为什么算法并不客观，可能暗藏歧视？算法决策在很多时候其实就是一种预测，
   用过去的数据预测未来的趋势。算法模型和数据输入决定着预测的结果。因此，
   这两个要素也就成为算法歧视的主要来源。一方面，算法在本质上是“以数学方
   式或者计算机代码表达的意见”，包括其设计、目的、成功标准、数据使用等等
   都是设计者、开发者的主观选择，设计者和开发者可能将自己所怀抱的偏见嵌入
   算法系统。另一方面，数据的有效性、准确性，也会影响整个算法决策和预测的
   准确性。比如，数据是社会现实的反映，训练数据本身可能是歧视性的，用这样
   的数据训练出来的AI系统自然也会带上歧视的影子。
   比如，数据可能是不正确、不完整或者过时的，带来所谓的“垃圾进，垃圾
   出”的现象；更进一步，如果一个AI系统依赖多数学习，自然不能兼容少数族
   裔的利益。此外，算法歧视可能是具有自我学习和适应能力的算法在交互过程
   中习得的，AI系统在与现实世界交互过程中，可能没法区别什么是歧视，什
   么不是歧视。
   几个例子：
     泰坦尼克号让妇女和儿童离开，因为他们更多对人类的未来有更大帮助，可以
     更好的扩大和继承人类的知识和智慧。
     ![在这里插入图片描述](https://img-blog.csdnimg.cn/20181230230635275.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzI2ODM5Mw==,size_16,color_FFFFFF,t_70)
     扳道工困境中，要具体分析哪一边对人类未来的知识和智慧可能的贡献大。
     ![在这里插入图片描述](https://img-blog.csdnimg.cn/20181230230907300.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzI2ODM5Mw==,size_16,color_FFFFFF,t_70)

本文主要内容来自知乎关于AI发展伦理问题的讨论。
